{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student ID: 2487190G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA Test (Initial run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import statistics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import RFE, SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import LeaveOneGroupOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels_df=pd.read_csv(\"labels.csv\",header=None)\n",
    "data_features_df=pd.read_csv(\"data.csv\", header=None)\n",
    "data_labels_df.columns=['labels']\n",
    "data_columns_list=list(pd.read_csv(\"feature_names.csv\"))\n",
    "\n",
    "data_features_df.columns=data_columns_list #merging the columns file and data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to get generate a final validation dataset for final classifier comparison\n",
    "#it uses the same the concept of LeaveOneGroupOut\n",
    "\n",
    "def custom_train_test_split(x,y,split_group_size=3,random_state=123):\n",
    "    \n",
    "    '''\n",
    "    Parameters:\n",
    "    x =>Feature (dataframe)\n",
    "    y =>Label (dataframe)\n",
    "    split_group_size => The groups we want in our validation set\n",
    "    random_state => random.seed()\n",
    "    \n",
    "    ----------------------------------------------------------------------------\n",
    "    This function splits the data into test and train splits. This works like Leave One Group Out concept where it doesn't\n",
    "    split random rows but splits random groups.\n",
    "    \n",
    "    This function ensures that the test set will have both the classes (1s and 0s).\n",
    "    \n",
    "    -----------------------------------------------------------------------------\n",
    "    Returns dataframes and a variable\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        \n",
    "    random.seed(random_state)  \n",
    "    choice_list=[x for x in range(18)]\n",
    "    groups=[x for x in range(18)]*10 #for using in LeaveOneGroupOut #180 length\n",
    "    groups.sort()\n",
    "    groups=np.array(groups).reshape(180,1) #groups.sort is sorted list which then converted to numpy array\n",
    "    groups_df=pd.DataFrame(groups,columns=['groups'])\n",
    "    \n",
    "    main_data_df=pd.concat([x,y], axis=1) #it contains features with last column as target variable\n",
    "    main_data_df=pd.concat([main_data_df,groups_df],axis=1) #the last column is the groups column now\n",
    "    \n",
    "    test_df=pd.DataFrame()\n",
    "    train_df=pd.DataFrame()\n",
    "    \n",
    "    #Objective is to always select both the classes in Test data:\n",
    "    \n",
    "    if split_group_size==2:\n",
    "        #selecting positive class for the test data\n",
    "        temp_df=main_data_df[main_data_df['labels']==1] #selecting a portion of the main_df where labels column are 1\n",
    "        random_patient=random.choice(list(set(temp_df['groups'].values.tolist()))) #selecting the group randomly  from temp_df and randomly picking a group\n",
    "        test_df=pd.concat([temp_df[temp_df['groups']==random_patient],test_df], axis=0) #this selects that random patient rows and store in test_df\n",
    "        main_data_df.drop(main_data_df[main_data_df['groups']==random_patient].index, inplace=True)\n",
    "        #this line will remove that patient rows from main_data_df. This patient is a POSITIVE class\n",
    "        \n",
    "        #selecting negative class for the test data \n",
    "        temp_df=main_data_df[main_data_df['labels']==0] #selecting a portion of the main_df where labels column are 0\n",
    "        random_patient=random.choice(list(set(temp_df['groups'].values.tolist()))) #selecting the group randomly  from temp_df and randomly picking a group\n",
    "        test_df=pd.concat([temp_df[temp_df['groups']==random_patient],test_df], axis=0) #this selects that random patient rows and store in test_df\n",
    "        main_data_df.drop(main_data_df[main_data_df['groups']==random_patient].index, inplace=True)\n",
    "        #this line will remove that patient rows from main_data_df. This patient is a NEGATIVE class\n",
    "        \n",
    "    else:\n",
    "        #must select atleast one 1 class and one 0 class\n",
    "        #selecting positive class for the test data\n",
    "        temp_df=main_data_df[main_data_df['labels']==1] #selecting a portion of the main_df where labels column are 1\n",
    "        random_patient=random.choice(list(set(temp_df['groups'].values.tolist()))) #selecting the group randomly  from temp_df and randomly picking a group\n",
    "        test_df=pd.concat([temp_df[temp_df['groups']==random_patient],test_df], axis=0) #this selects that random patient rows and store in test_df\n",
    "        main_data_df.drop(main_data_df[main_data_df['groups']==random_patient].index, inplace=True)\n",
    "        #this line will remove that patient rows from main_data_df. This patient is a POSITIVE class\n",
    "        \n",
    "        #selecting negative class for the test data \n",
    "        temp_df=main_data_df[main_data_df['labels']==0] #selecting a portion of the main_df where labels column are 0\n",
    "        random_patient=random.choice(list(set(temp_df['groups'].values.tolist()))) #selecting the group randomly  from temp_df and randomly picking a group\n",
    "        test_df=pd.concat([temp_df[temp_df['groups']==random_patient],test_df], axis=0) #this selects that random patient rows and store in test_df\n",
    "        main_data_df.drop(main_data_df[main_data_df['groups']==random_patient].index, inplace=True)\n",
    "        #this line will remove that patient rows from main_data_df. This patient is a NEGATIVE class\n",
    "        \n",
    "        #Two groups selected for test data already above, remaining group(s) will be random selection:\n",
    "           \n",
    "    \n",
    "        for i in range(split_group_size-2):\n",
    "            random_pick=random.choice(list(set(main_data_df['groups'].values.tolist())))  #picking a random number from the groups column\n",
    "            #choice_list.remove(random_pick) #removing it from the choice_list so that in the next iteration the same number doesn't get picked\n",
    "            test_df=pd.concat((main_data_df[main_data_df['groups']==random_pick],test_df)) #concatenating X_test on top of X_test in each iteration to build the test set\n",
    "            main_data_df.drop(main_data_df[main_data_df['groups']==random_pick].index, inplace=True) #Permanently dropping this patient group in each iteration from the main_data_df\n",
    "            \n",
    "    \n",
    "     #now main_data_df won't have the test patient groups data in it, so it is our train set dataframe\n",
    "        \n",
    "        \n",
    "        \n",
    "    X_train_df=main_data_df.iloc[:,0:-2] #the last two columns are target label and a group column added in the previous lines \n",
    "    Y_train_df=main_data_df.iloc[:,-2] #the 2nd last column is target data\n",
    "    X_test_df= test_df.iloc[:,0:-2]\n",
    "    Y_test_df= test_df.iloc[:,-2]\n",
    "    groups=    18-split_group_size\n",
    "    #groups variable will be fed to LeaveOneGroupOut while training \n",
    "    \n",
    "    X_train_df.reset_index(drop=True,inplace=True) #resets the index, drops the old index as it will get added as a column and order is important \n",
    "    Y_train_df.reset_index(drop=True,inplace=True)\n",
    "    X_test_df.reset_index(drop=True, inplace=True)\n",
    "    Y_test_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    return X_train_df,X_test_df,Y_train_df,Y_test_df,groups\n",
    "\n",
    "\n",
    "\n",
    "#custom_train_test_split(x=data_features_df,y=data_labels_df,split_group_size=2,random_state=123)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below cell performs cross validation to determine which is the best feature combination from ANOVA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy collected at every iteration:[0.74, 0.7066666666666667, 0.7666666666666667, 0.7733333333333333, 0.7733333333333333, 0.76, 0.7733333333333333, 0.7933333333333333, 0.7866666666666666, 0.78, 0.78, 0.7533333333333333, 0.7533333333333333, 0.7666666666666667, 0.78, 0.7733333333333333, 0.7666666666666667, 0.7666666666666667, 0.78, 0.7666666666666667, 0.7533333333333333, 0.76, 0.76, 0.7733333333333333, 0.7866666666666667, 0.7866666666666667, 0.7866666666666667, 0.7866666666666667, 0.78, 0.8, 0.8066666666666666, 0.8133333333333334, 0.8266666666666667, 0.8266666666666667, 0.8333333333333334, 0.8266666666666667, 0.8133333333333334, 0.8133333333333334, 0.8066666666666666, 0.8066666666666666, 0.8133333333333334, 0.8133333333333334, 0.8066666666666666]\n",
      "\n",
      "\n",
      "The best rank is 350 at highest accuracy (0.833) from LOGO cross-validation \n",
      "\n",
      "\n",
      "Total time taken to run ANOVA test:\n",
      "\n",
      "00:00:03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The accuracy formula accuracy=(tp+tn)/(tp+tn+fp+fn) or the accuracy_score(Y_test_cv.tolist(),y_predict.tolist())\\ngive the same result, so using anyone of them'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''this custom splitting ensures that the test set always has both the classes. This is important because otherwise accuracy\n",
    "will be suffer\n",
    "'''\n",
    "start_time=time.time() \n",
    "\n",
    "X_train_val, X_test_val, Y_train_val,Y_test_val,groups=\\\n",
    "                       custom_train_test_split(x=data_features_df,y=data_labels_df,split_group_size=3,random_state=123)\n",
    "\n",
    "group_cv=[x for x in range(groups)]*10\n",
    "group_cv.sort()\n",
    "\n",
    "logistic_clf=LogisticRegression()\n",
    "\n",
    "logo_cv = LeaveOneGroupOut()\n",
    "\n",
    "avg_accuracy_list_logistic=avg_score_list_logistic=[]\n",
    "\n",
    "ANOVA_selected_columns=dict()\n",
    "\n",
    "ANOVA_ranks=[x for x in range(10,433,10)]\n",
    "for rank in ANOVA_ranks:\n",
    "    \n",
    "    ANOVA=SelectKBest(f_classif,k=rank)\n",
    "    new_features_ANOVA=ANOVA.fit_transform(X_train_val, Y_train_val)\n",
    "    \n",
    "    ANOVA_selected_columns[rank]=ANOVA.get_support(indices=True) #keeps track of columns chosen SelectKBest in the dictionary\n",
    "    \n",
    "    accuracy_list=[] \n",
    "    score_list=[]\n",
    "    \n",
    "    for train, test in logo_cv.split(new_features_ANOVA, Y_train_val.values, group_cv):\n",
    "       \n",
    "        X_train_cv, X_test_cv = new_features_ANOVA[train], new_features_ANOVA[test]\n",
    "        Y_train_cv, Y_test_cv = Y_train_val.values[train], Y_train_val.values[test]     \n",
    "        \n",
    "        logistic_clf.fit(X_train_cv,Y_train_cv)\n",
    "        \n",
    "        y_predict=logistic_clf.predict(X_test_cv)\n",
    "        \n",
    "        tp=fp=tn=fn=i=0\n",
    "        \n",
    "        while i < len(y_predict):\n",
    "            if Y_test_cv[i]==1 and y_predict[i]==1:             #corresponding actual y=1 and prediction =1 then it is True+ve\n",
    "                tp=tp+1\n",
    "            elif Y_test_cv[i]==0 and y_predict[i]==1:           #corresponding actual y=0 and prediction =1 then it is False+ve\n",
    "                fp=fp+1\n",
    "            elif Y_test_cv[i]==0 and y_predict[i]==0:           #corresponding actual y=0 and prediction =0 then it is True-ve\n",
    "                tn=tn+1\n",
    "            else:                                               #corresponding actual y=1 and prediction =0 then it is False-ve\n",
    "                fn=fn+1\n",
    "            i+=1\n",
    "            \n",
    "        accuracy=(tp+tn)/(tp+tn+fp+fn)                          #accuracy formula\n",
    "        accuracy_list.append(accuracy)                          #storing history data in accuracy_list\n",
    "        #accuracy_scores=accuracy_score(Y_test_cv.tolist(),y_predict.tolist())\n",
    "        #score_list.append(accuracy_scores)\n",
    "        \n",
    "    avg_accuracy_list_logistic.append(statistics.mean(accuracy_list))    #finally adding the average accuracy of each model\n",
    "    #avg_score_list_logistic.append(statistics.mean(score_list))    \n",
    "\n",
    "best_rank=ANOVA_ranks[avg_accuracy_list_logistic.index(max(avg_accuracy_list_logistic))]\n",
    "\n",
    "\n",
    "print(f\"The average accuracy collected at every iteration:{avg_accuracy_list_logistic}\")\n",
    "print(\"\\n\")\n",
    "print(f\"The best rank is {best_rank} at highest accuracy ({max(avg_accuracy_list_logistic):.3f}) from LOGO cross-validation \")\n",
    "print(\"\\n\")\n",
    "\n",
    "elapsed_time=time.time()-start_time\n",
    "\n",
    "#print(f\"The best rank is {best_rank_score} at highest accuracy ({max(avg_score_list_logistic):.3f}) from LOGO cross-validation \")\n",
    "\n",
    "print(\"Total time taken to run ANOVA test:\\n\")\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "'''The accuracy formula accuracy=(tp+tn)/(tp+tn+fp+fn) or the accuracy_score(Y_test_cv.tolist(),y_predict.tolist())\n",
    "give the same result, so using anyone of them'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The best feature combo is 350 in total. Moving to Mutual Infrmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
